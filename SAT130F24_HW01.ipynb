{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddab1b34",
   "metadata": {},
   "source": [
    "Pre-Lecture HOMEWORK Question 1: Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec96109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aafd55f1",
   "metadata": {},
   "source": [
    "Pre-Lecture HOMEWORK Question 2: Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a pandas DataFrame has, and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94869dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a788493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 391\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = data.shape\n",
    "print(f'Number of rows: {num_rows}')\n",
    "print(f'Number of columns: {num_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602ebd9",
   "metadata": {},
   "source": [
    "OBSERVATION: observations are the individual data entries of my data set. ie. the individual names of each animal crossing character\n",
    "VARIBALES: variables are like bags that hold/describe each observation within my dataset. ie. the column 'name' holds each name of the animal crossing characters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90d34f",
   "metadata": {},
   "source": [
    "Pre-Lecture HOMEWORK Question 3:Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695bc84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n"
     ]
    }
   ],
   "source": [
    "summary = df.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1c7a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of      row_n        id      name  gender    species birthday personality  \\\n",
      "0        2   admiral   Admiral    male       bird     1-27      cranky   \n",
      "1        3   agent-s   Agent S  female   squirrel      7-2       peppy   \n",
      "2        4     agnes     Agnes  female        pig     4-21        uchi   \n",
      "3        6        al        Al    male    gorilla    10-18        lazy   \n",
      "4        7   alfonso   Alfonso    male  alligator      6-9        lazy   \n",
      "..     ...       ...       ...     ...        ...      ...         ...   \n",
      "386    475    winnie    Winnie  female      horse     1-31       peppy   \n",
      "387    477  wolfgang  Wolfgang    male       wolf    11-25      cranky   \n",
      "388    480      yuka      Yuka  female      koala     7-20      snooty   \n",
      "389    481      zell      Zell    male       deer      6-7        smug   \n",
      "390    483    zucker    Zucker    male    octopus      3-8        lazy   \n",
      "\n",
      "                song    phrase            full_id  \\\n",
      "0         Steep Hill   aye aye   villager-admiral   \n",
      "1            DJ K.K.  sidekick   villager-agent-s   \n",
      "2         K.K. House   snuffle     villager-agnes   \n",
      "3         Steep Hill   Ayyeeee        villager-al   \n",
      "4        Forest Life  it'sa me   villager-alfonso   \n",
      "..               ...       ...                ...   \n",
      "386         My Place    hay-OK    villager-winnie   \n",
      "387        K.K. Song   snarrrl  villager-wolfgang   \n",
      "388     Soulful K.K.   tsk tsk      villager-yuka   \n",
      "389         K.K. D&B     pronk      villager-zell   \n",
      "390  Spring Blossoms     bloop    villager-zucker   \n",
      "\n",
      "                                                   url  \n",
      "0    https://villagerdb.com/images/villagers/thumb/...  \n",
      "1    https://villagerdb.com/images/villagers/thumb/...  \n",
      "2    https://villagerdb.com/images/villagers/thumb/...  \n",
      "3    https://villagerdb.com/images/villagers/thumb/...  \n",
      "4    https://villagerdb.com/images/villagers/thumb/...  \n",
      "..                                                 ...  \n",
      "386  https://villagerdb.com/images/villagers/thumb/...  \n",
      "387  https://villagerdb.com/images/villagers/thumb/...  \n",
      "388  https://villagerdb.com/images/villagers/thumb/...  \n",
      "389  https://villagerdb.com/images/villagers/thumb/...  \n",
      "390  https://villagerdb.com/images/villagers/thumb/...  \n",
      "\n",
      "[391 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "summary1 = data.describe()\n",
    "print(data.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3a6c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "personality_counts = df['personality'].value_counts()\n",
    "print(personality_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c6bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id      name  gender    species birthday personality  \\\n",
      "0     admiral   Admiral    male       bird     1-27      cranky   \n",
      "1     agent-s   Agent S  female   squirrel      7-2       peppy   \n",
      "2       agnes     Agnes  female        pig     4-21        uchi   \n",
      "3          al        Al    male    gorilla    10-18        lazy   \n",
      "4     alfonso   Alfonso    male  alligator      6-9        lazy   \n",
      "..        ...       ...     ...        ...      ...         ...   \n",
      "386    winnie    Winnie  female      horse     1-31       peppy   \n",
      "387  wolfgang  Wolfgang    male       wolf    11-25      cranky   \n",
      "388      yuka      Yuka  female      koala     7-20      snooty   \n",
      "389      zell      Zell    male       deer      6-7        smug   \n",
      "390    zucker    Zucker    male    octopus      3-8        lazy   \n",
      "\n",
      "                song    phrase            full_id  \\\n",
      "0         Steep Hill   aye aye   villager-admiral   \n",
      "1            DJ K.K.  sidekick   villager-agent-s   \n",
      "2         K.K. House   snuffle     villager-agnes   \n",
      "3         Steep Hill   Ayyeeee        villager-al   \n",
      "4        Forest Life  it'sa me   villager-alfonso   \n",
      "..               ...       ...                ...   \n",
      "386         My Place    hay-OK    villager-winnie   \n",
      "387        K.K. Song   snarrrl  villager-wolfgang   \n",
      "388     Soulful K.K.   tsk tsk      villager-yuka   \n",
      "389         K.K. D&B     pronk      villager-zell   \n",
      "390  Spring Blossoms     bloop    villager-zucker   \n",
      "\n",
      "                                                   url  \n",
      "0    https://villagerdb.com/images/villagers/thumb/...  \n",
      "1    https://villagerdb.com/images/villagers/thumb/...  \n",
      "2    https://villagerdb.com/images/villagers/thumb/...  \n",
      "3    https://villagerdb.com/images/villagers/thumb/...  \n",
      "4    https://villagerdb.com/images/villagers/thumb/...  \n",
      "..                                                 ...  \n",
      "386  https://villagerdb.com/images/villagers/thumb/...  \n",
      "387  https://villagerdb.com/images/villagers/thumb/...  \n",
      "388  https://villagerdb.com/images/villagers/thumb/...  \n",
      "389  https://villagerdb.com/images/villagers/thumb/...  \n",
      "390  https://villagerdb.com/images/villagers/thumb/...  \n",
      "\n",
      "[391 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "non_numerical_variables = data.select_dtypes(exclude='number')\n",
    "print(non_numerical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8f4180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'gender', 'species', 'birthday', 'personality', 'song', 'phrase', 'full_id', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(non_numerical_variables.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaf25b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.select_dtypes(include='number').isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f550a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(missing_values[missing_values>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f9e6b",
   "metadata": {},
   "source": [
    "4. If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by df.shape and what is reported by df.describe() with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c83f6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_data_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "data = pd.read_csv(new_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b12957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numerical_vars = data.select_dtypes(exclude='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fc72dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Numerical Variables:\n",
      "['sex', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone']\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-Numerical Variables:\")\n",
    "print(non_numerical_vars.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a546a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_new_values = data.select_dtypes(include='number').isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca6284a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Missing_Values:\n",
      "survived      0\n",
      "pclass        0\n",
      "age         177\n",
      "sibsp         0\n",
      "parch         0\n",
      "fare          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"New_Missing_Values:\")\n",
    "print(missing_new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c99ad8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2f5d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset has 891 rows and 15columns.\n"
     ]
    }
   ],
   "source": [
    "print(f'the dataset has {shape[0]} rows and {shape[1]}columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38e0b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_titanic = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32ef2989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "print(summary_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6d925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b83b9d2",
   "metadata": {},
   "source": [
    "SUMMARY OF PRELECTURE HW - chatbot\n",
    "1.Dataset Overview: You asked about the Animal Crossing characters dataset and how to find the number of rows and columns in it using pandas.\n",
    "\n",
    "2.Observations and Variables: We discussed the concepts of observations (individual records/rows) and variables (attributes/columns) in a dataset.\n",
    "\n",
    "3.Pandas DataFrame: I explained what a pandas DataFrame is, highlighting its tabular structure, indexing, and functionalities for data manipulation.\n",
    "\n",
    "4.Summarizing Data: You inquired about how to provide simple summaries of columns, and I suggested using df.describe() for statistical summaries and value_counts() for unique counts in specific columns.\n",
    "\n",
    "5.Error Explanation: You encountered the error AttributeError: 'DataFrame' object has no attribute 'split', and I explained that this occurs when attempting to use a string method on a DataFrame instead of a Series or string.\n",
    "\n",
    "6.Output Interpretation: We discussed the meaning of Series([], dtype: int64), indicating an empty Series, and how to find non-numerical variables and missing values in a dataset.\n",
    "\n",
    "7.Replacing Datasets: I showed how to replace an old dataset with a new one by loading it into the same variable.\n",
    "\n",
    "8.Understanding df.shape and df.describe(): We clarified the differences between these two, focusing on the number of columns analyzed and the values reported in the 'count' column.\n",
    "\n",
    "9.Attributes vs. Methods: Finally, I explained the difference between attributes (variables associated with an object) and methods (functions defined within a class that operate on the object).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967e28d",
   "metadata": {},
   "source": [
    "Post-Lecure HW Question 6: The df.describe() method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics\n",
    "\n",
    "count: the number of non-null entries\n",
    "mean: the average value\n",
    "std: the standard deviation, indicating variability/spread of your data set\n",
    "min: the minimum value within your data set\n",
    "25%: the value that falls beneath the 25th percentile of the data set\n",
    "50%: the value of the middle of the data set\n",
    "75%: the valye that falls beneath the 75th percentile of the data set \n",
    "max: the maximum value within your data set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d5089",
   "metadata": {},
   "source": [
    "Post-Lecture HW \n",
    "Question 7a) Provide an example of a \"use case\" in which using df.dropna() might be peferred over using del df['col']\n",
    "\n",
    "A time where one may want to use df.dropna over using del df['col'] is when thy want their data set to be completely cleared and clean of any rows that have NaNs \n",
    ", this may be usefull for a system that cannot run or function properly when there is NANs in the data set \n",
    "\n",
    "Question 7b) Provide an example of \"the opposite use case\" in which using del df['col'] might be preferred over using df.dropna()\n",
    "using del df['col'] may be usefull over df.dropna when we are analyzing a certain data set and want to keep all of the columns for extra information when trying to analyze a data set, before running a certain function \n",
    "\n",
    "Question 7c)Discuss why applying del df['col'] before df.dropna() when both are used together could be important\n",
    "Using del df'['col'] before applying df.dropna() will help preserve as much data as possible for analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a0fd3",
   "metadata": {},
   "source": [
    "Question 7d) Remove all missing data from one of the datasets you're considering using some combination of del df['col'] and/or df.dropna() and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7cdd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'OrderID': [1, 2, 3, 4, 5],\n",
    "    'CustomerID': [101, 102, np.nan, 104, 105],\n",
    "    'Product': ['Apples', 'Bananas', 'Oranges', 'Grapes', 'Pineapples'],\n",
    "    'Quantity': [5, np.nan, 10, 7, 3],\n",
    "    'Price': [1.50, 0.75, 2.00, np.nan, 3.00]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1773c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OrderID  CustomerID     Product  Quantity  Price\n",
      "0        1       101.0      Apples       5.0   1.50\n",
      "1        2       102.0     Bananas       NaN   0.75\n",
      "2        3         NaN     Oranges      10.0   2.00\n",
      "3        4       104.0      Grapes       7.0    NaN\n",
      "4        5       105.0  Pineapples       3.0   3.00\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fc4b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OrderID  CustomerID     Product  Quantity  Price\n",
      "0        1       101.0      Apples       5.0    1.5\n",
      "4        5       105.0  Pineapples       3.0    3.0\n"
     ]
    }
   ],
   "source": [
    "clean_data = df.dropna()\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4882f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ce4b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OrderID  CustomerID     Product  Quantity\n",
      "0        1       101.0      Apples       5.0\n",
      "1        2       102.0     Bananas       NaN\n",
      "2        3         NaN     Oranges      10.0\n",
      "3        4       104.0      Grapes       7.0\n",
      "4        5       105.0  Pineapples       3.0\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423aea8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case it is more helpful for me to only use df.dropna to drop certain rows in my data set, this way I can still keep some information about the columns with NaNs instead of losing the entire thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4569bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the URL\n",
    "new_fourth_url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv'\n",
    "dogs_df = pd.read_csv(url)\n",
    "data = pd.read_csv(new_fourth_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ae53f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in the dataset:\n",
      "Index                0\n",
      " Height(Inches)\"     0\n",
      " \"Weight(Pounds)\"    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(dogs_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0bae93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Index   Height(Inches)\"   \"Weight(Pounds)\"\n",
      "0        1             65.78             112.99\n",
      "1        2             71.52             136.49\n",
      "2        3             69.40             153.03\n",
      "3        4             68.22             142.34\n",
      "4        5             67.79             144.30\n",
      "..     ...               ...                ...\n",
      "195    196             65.80             120.84\n",
      "196    197             66.11             115.78\n",
      "197    198             68.24             128.30\n",
      "198    199             68.02             127.47\n",
      "199    200             71.39             127.88\n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dogs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e1534912",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_stats = dogs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "166c4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Index   Height(Inches)\"   \"Weight(Pounds)\"\n",
      "count  200.000000        200.000000         200.000000\n",
      "mean   100.500000         67.949800         127.221950\n",
      "std     57.879185          1.940363          11.960959\n",
      "min      1.000000         63.430000          97.900000\n",
      "25%     50.750000         66.522500         119.895000\n",
      "50%    100.500000         67.935000         127.875000\n",
      "75%    150.250000         69.202500         136.097500\n",
      "max    200.000000         73.900000         158.960000\n"
     ]
    }
   ],
   "source": [
    "print(dog_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768592c",
   "metadata": {},
   "source": [
    "The reason the count value from df.describe is different from the count value using df.groupby(\"col1\")[\"col2\"].describe() is because the count value for the aforementioned code is only counting the number of non-null entries in relation to column two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862e3fdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset from the URL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m new_fourth_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dogs_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(new_fourth_url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the URL\n",
    "new_fourth_url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv'\n",
    "dogs_df = pd.read_csv(url)\n",
    "data = pd.read_csv(new_fourth_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8038a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_data_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(new_data_url)\n",
    "data = pd.read_csv(new_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644de689",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'col1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtitanic_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'col1'"
     ]
    }
   ],
   "source": [
    "titanic_df.groupby(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7557c687",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SSex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtitanic_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SSex'"
     ]
    }
   ],
   "source": [
    "titanic_df.groupby(\"SSex\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966b798",
   "metadata": {},
   "source": [
    "While intentionally adding in errors and using different engines, google vs chatbot, to introduce the issue and try to find the solution I found that it is much easier to use the chatbot because it can give solutions pertaining to your speicfic data sat and its memory can help provide solutions based on your previous questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801b5ba",
   "metadata": {},
   "source": [
    "9. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?\n",
    "\n",
    "somewhat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interaction Summary\n",
    "Introduction to Data Handling: You asked about various pandas functions and how to manipulate DataFrames in Python.\n",
    "\n",
    "Descriptive Statistics: You inquired about df.describe(), which provides statistical summaries of numerical columns in a DataFrame.\n",
    "\n",
    "Handling Missing Data: We discussed functions like df.dropna() and how to manage missing values in your dataset.\n",
    "\n",
    "Deleting Columns: You learned about deleting columns using del df['col'] and the syntax for deleting multiple columns.\n",
    "\n",
    "Working with Datasets: You requested datasets, specifically a dog dataset with missing values accessible via a URL. We explored how to load a dataset using pd.read_csv().\n",
    "\n",
    "Grouping Data: You asked about using df.groupby() to summarize data, and we discussed how to replace generic column names with actual names from your dataset.\n",
    "\n",
    "Syntax Errors: You encountered a few errors (e.g., NameError, KeyError, SyntaxError), which we addressed by correcting the code snippets and ensuring proper syntax.\n",
    "\n",
    "Final Example: We provided a final code example to load a Titanic dataset, inspect its columns, and group the data to generate descriptive statistics.\n",
    "\n",
    "If you have any further questions or need clarification on any topic, feel free to ask!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
